python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v42 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=5 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=1 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v43 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=5 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=1 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=2.0 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v44 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=5 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=1 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.0 \
    --do_rdrop \
    --rdrop_weight=0.1 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v45 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=5 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --fgm_epsilon=0.5 \
    --num_lstm_layers=1 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.0 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v46 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=5 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.0 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v47 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=5 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=1 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v48 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=7 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=1 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v49 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=6 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=1 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v50 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3.5e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=5 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=1 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --seed=42


python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v51 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=5 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --seed=42


python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v52 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=5 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.8 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v54 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=6 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.810 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v55 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=6 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.810 \
    --awp_lr=0.001 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v56 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=6 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.810 \
    --awp_lr=0.001 \
    --do_rdrop \
    --rdrop_weight=0.1 \
    --rdrop_epoch=2 \
    --seed=42

#---do-swa
python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v56 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_post_swa \
    --swa_model_dir=../data/tmp_data/gaiic_nezha_experiment_bert_base_fold0_gp_v2_pre_v62 \
    --swa_start=2 \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=6 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.810 \
    --awp_lr=0.001 \
    --do_rdrop \
    --rdrop_weight=0.1 \
    --rdrop_epoch=2 \
    --seed=42

## predict
python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v56 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_predict \
    --eval_checkpoint_path=../data/tmp_data/gaiic_nezha_experiment_bert_base_fold0_gp_v2_pre_v62/checkpoint-0.81773-20250\
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=6 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.810 \
    --awp_lr=0.001 \
    --do_rdrop \
    --rdrop_weight=0.1 \
    --rdrop_epoch=2 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v57 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=6 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.810 \
    --awp_lr=0.1 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v59 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=6 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.810 \
    --awp_lr=0.1 \
    --do_rdrop \
    --rdrop_weight=0.3 \
    --rdrop_epoch=2 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v60 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=6 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.810 \
    --awp_lr=0.1 \
    --do_rdrop \
    --rdrop_weight=0.4 \
    --rdrop_epoch=1 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v61 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=8 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.810 \
    --awp_lr=0.1 \
    --do_rdrop \
    --rdrop_weight=0.4 \
    --rdrop_epoch=1 \
    --seed=42

python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v63 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.-1.jsonl \
    --eval_input_file=dev.-1.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=10 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.810 \
    --awp_lr=0.1 \
    --do_rdrop \
    --rdrop_weight=0.4 \
    --rdrop_epoch=1 \
    --seed=42

### 提交校验
#python exp_gaiic_global_pointer_v2.py \
#    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v62 \
#    --task_name=gaiic \
#    --model_type=nezha \
#    --do_lower_case \
#    --pretrained_model_path=../data/submission/checkpoint-7800 \
#    --data_dir=../data/tmp_data/10_folds_data/ \
#    --train_input_file=train.0.jsonl \
#    --eval_input_file=dev.0.jsonl \
#    --output_dir=../data/tmp_data/ \
#    --do_predict_test \
#    --test_input_file=../test_submit_dev_0.txt \
#    --eval_checkpoint_path=../data/best_model \
#    --submit_file_path=../results.txt \
#    --evaluate_during_training \
#    --train_max_seq_length=128 \
#    --eval_max_seq_length=128 \
#    --test_max_seq_length=128 \
#    --per_gpu_train_batch_size=16 \
#    --per_gpu_eval_batch_size=32 \
#    --per_gpu_test_batch_size=32 \
#    --gradient_accumulation_steps=1 \
#    --learning_rate=3e-5 \
#    --other_learning_rate=1e-3 \
#    --weight_decay=0.001 \
#    --scheduler_type=cosine \
#    --base_model_name=bert \
#    --warmup_proportion=0.1 \
#    --max_grad_norm=1.0 \
#    --num_train_epochs=10 \
#    --use_rope \
#    --do_lstm \
#    --do_fgm \
#    --num_lstm_layers=2 \
#    --adam_epsilon=1e-8 \
#    --post_lstm_dropout=0.5 \
#    --inner_dim=64 \
#    --loss_type=pcl \
#    --pcl_epsilon=2.5 \
#    --pcl_alpha=1.5 \
#    --do_awp \
#    --awp_f1=0.810 \
#    --awp_lr=0.1 \
#    --do_rdrop \
#    --rdrop_weight=0.4 \
#    --rdrop_epoch=1 \
#    --seed=42

# 全量数据
python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v64 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.all.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=8 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.810 \
    --awp_lr=0.1 \
    --do_rdrop \
    --rdrop_weight=0.4 \
    --rdrop_epoch=1 \
    --seed=42


python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v56 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/submission/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_post_swa \
    --swa_model_dir=../data/tmp_data/gaiic_nezha_experiment_bert_base_fold0_gp_v2_pre_v64 \
    --swa_start=2 \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=6 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.810 \
    --awp_lr=0.001 \
    --do_rdrop \
    --rdrop_weight=0.1 \
    --rdrop_epoch=2 \
    --seed=42


python exp_gaiic_global_pointer_v2.py \
    --experiment_code=experiment_bert_base_fold0_gp_v2_pre_v57_1 \
    --task_name=gaiic \
    --model_type=nezha \
    --do_lower_case \
    --pretrained_model_path=../data/public_data/checkpoint-7800 \
    --data_dir=../data/tmp_data/10_folds_data/ \
    --train_input_file=train.0.jsonl \
    --eval_input_file=dev.0.jsonl \
    --output_dir=../data/tmp_data/ \
    --do_train \
    --evaluate_during_training \
    --train_max_seq_length=128 \
    --eval_max_seq_length=128 \
    --test_max_seq_length=128 \
    --per_gpu_train_batch_size=16 \
    --per_gpu_eval_batch_size=32 \
    --per_gpu_test_batch_size=32 \
    --gradient_accumulation_steps=1 \
    --learning_rate=3.5e-5 \
    --other_learning_rate=1e-3 \
    --weight_decay=0.001 \
    --scheduler_type=cosine \
    --base_model_name=bert \
    --warmup_proportion=0.1 \
    --max_grad_norm=1.0 \
    --num_train_epochs=5 \
    --use_rope \
    --do_lstm \
    --do_fgm \
    --num_lstm_layers=2 \
    --adam_epsilon=1e-8 \
    --post_lstm_dropout=0.5 \
    --inner_dim=64 \
    --loss_type=pcl \
    --pcl_epsilon=2.5 \
    --pcl_alpha=1.5 \
    --do_awp \
    --awp_f1=0.810 \
    --awp_lr=0.1 \
    --seed=42